{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-12T12:12:45.885571Z","iopub.execute_input":"2025-04-12T12:12:45.885908Z","iopub.status.idle":"2025-04-12T12:12:46.229129Z","shell.execute_reply.started":"2025-04-12T12:12:45.885880Z","shell.execute_reply":"2025-04-12T12:12:46.228089Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n#!pip install -U -q \"google-genai==1.7.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T12:12:46.230776Z","iopub.execute_input":"2025-04-12T12:12:46.231440Z","iopub.status.idle":"2025-04-12T12:12:49.951968Z","shell.execute_reply.started":"2025-04-12T12:12:46.231403Z","shell.execute_reply":"2025-04-12T12:12:49.950666Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\ngenai.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T12:12:49.953151Z","iopub.execute_input":"2025-04-12T12:12:49.953472Z","iopub.status.idle":"2025-04-12T12:12:51.560120Z","shell.execute_reply.started":"2025-04-12T12:12:49.953438Z","shell.execute_reply":"2025-04-12T12:12:51.559428Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'1.7.0'"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"### Set up your API key\n\nTo run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n\nIf you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n\nTo make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T12:13:35.005483Z","iopub.execute_input":"2025-04-12T12:13:35.005818Z","iopub.status.idle":"2025-04-12T12:13:35.486503Z","shell.execute_reply.started":"2025-04-12T12:13:35.005792Z","shell.execute_reply":"2025-04-12T12:13:35.485465Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Explore available models\n\nYou will be using the [`TunedModel.create`](https://ai.google.dev/api/tuning#method:-tunedmodels.create) API method to start the fine-tuning job and create your custom model. Find a model that supports it through the [`models.list`](https://ai.google.dev/api/models#method:-models.list) endpoint. You can also find more information about tuning models in [the model tuning docs](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=python).","metadata":{}},{"cell_type":"code","source":"for model in client.models.list():\n    if \"createTunedModel\" in model.supported_actions:\n        print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T12:13:59.960273Z","iopub.execute_input":"2025-04-12T12:13:59.960598Z","iopub.status.idle":"2025-04-12T12:14:00.217312Z","shell.execute_reply.started":"2025-04-12T12:13:59.960571Z","shell.execute_reply":"2025-04-12T12:14:00.216292Z"}},"outputs":[{"name":"stdout","text":"models/gemini-1.5-flash-001-tuning\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Download the dataset\n\nIn this activity, you will use the same newsgroups dataset that [you used to train a classifier in Keras](https://www.kaggle.com/code/markishere/day-2-classifying-embeddings-with-keras/). In this example you will use a fine-tuned Gemini model to achieve the same goal.\n\nThe [20 Newsgroups Text Dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) contains 18,000 newsgroups posts on 20 topics divided into training and test sets.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import fetch_20newsgroups\n\nnewsgroups_train = fetch_20newsgroups(subset=\"train\")\nnewsgroups_test = fetch_20newsgroups(subset=\"test\")\n\n# View list of class names for dataset\nnewsgroups_train.target_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T12:20:01.034681Z","iopub.execute_input":"2025-04-12T12:20:01.035035Z","iopub.status.idle":"2025-04-12T12:20:11.662930Z","shell.execute_reply.started":"2025-04-12T12:20:01.035003Z","shell.execute_reply":"2025-04-12T12:20:11.661984Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"**Here's what a single row looks like.**","metadata":{}},{"cell_type":"code","source":"print(newsgroups_train.data[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T12:22:29.815287Z","iopub.execute_input":"2025-04-12T12:22:29.815670Z","iopub.status.idle":"2025-04-12T12:22:29.820933Z","shell.execute_reply.started":"2025-04-12T12:22:29.815642Z","shell.execute_reply":"2025-04-12T12:22:29.819943Z"}},"outputs":[{"name":"stdout","text":"From: lerxst@wam.umd.edu (where's my thing)\nSubject: WHAT car is this!?\nNntp-Posting-Host: rac3.wam.umd.edu\nOrganization: University of Maryland, College Park\nLines: 15\n\n I was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is \nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\n\nThanks,\n- IL\n   ---- brought to you by your neighborhood Lerxst ----\n\n\n\n\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### Data Preprocessing\nThe raw data contains extra information such as email addresses and email headers that need to be removed.\n* The **preprocess_newsgroup_row** and **preprocess_newsgroup_data** functions are used to clean and format the data.\n* Finally, the data is stored as a nested DataFrame.","metadata":{}},{"cell_type":"code","source":"import email\nimport re\n\nimport pandas as pd\n\n\ndef preprocess_newsgroup_row(data):\n    # Extract only the subject and body\n    msg = email.message_from_string(data)\n    text = f\"{msg['Subject']}\\n\\n{msg.get_payload()}\"\n    # Strip any remaining email addresses\n    text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", text)\n    # Truncate the text to fit within the input limits\n    text = text[:40000]\n\n    return text\n\n\ndef preprocess_newsgroup_data(newsgroup_dataset):\n    # Put data points into dataframe\n    df = pd.DataFrame(\n        {\"Text\": newsgroup_dataset.data, \"Label\": newsgroup_dataset.target}\n    )\n    # Clean up the text\n    df[\"Text\"] = df[\"Text\"].apply(preprocess_newsgroup_row)\n    # Match label to target name index\n    df[\"Class Name\"] = df[\"Label\"].map(lambda l: newsgroup_dataset.target_names[l])\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T12:26:26.294946Z","iopub.execute_input":"2025-04-12T12:26:26.295646Z","iopub.status.idle":"2025-04-12T12:26:26.301670Z","shell.execute_reply.started":"2025-04-12T12:26:26.295617Z","shell.execute_reply":"2025-04-12T12:26:26.300801Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Apply preprocessing to training and test datasets\ndf_train = preprocess_newsgroup_data(newsgroups_train)\ndf_test = preprocess_newsgroup_data(newsgroups_test)\n\ndf_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T12:26:38.884630Z","iopub.execute_input":"2025-04-12T12:26:38.885040Z","iopub.status.idle":"2025-04-12T12:26:42.418661Z","shell.execute_reply.started":"2025-04-12T12:26:38.885010Z","shell.execute_reply":"2025-04-12T12:26:42.417789Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                                Text  Label  \\\n0  WHAT car is this!?\\n\\n I was wondering if anyo...      7   \n1  SI Clock Poll - Final Call\\n\\nA fair number of...      4   \n2  PB questions...\\n\\nwell folks, my mac plus fin...      4   \n3  Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr...      1   \n4  Re: Shuttle Launch Question\\n\\nFrom article <>...     14   \n\n              Class Name  \n0              rec.autos  \n1  comp.sys.mac.hardware  \n2  comp.sys.mac.hardware  \n3          comp.graphics  \n4              sci.space  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Label</th>\n      <th>Class Name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>WHAT car is this!?\\n\\n I was wondering if anyo...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SI Clock Poll - Final Call\\n\\nA fair number of...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PB questions...\\n\\nwell folks, my mac plus fin...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr...</td>\n      <td>1</td>\n      <td>comp.graphics</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Re: Shuttle Launch Question\\n\\nFrom article &lt;&gt;...</td>\n      <td>14</td>\n      <td>sci.space</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"### Sampling data\nTo reduce the size of the data and save time and resources, only a certain number of samples are selected from each class.\nThis is done using the **sample_data** function.","metadata":{}},{"cell_type":"code","source":"def sample_data(df, num_samples, classes_to_keep):\n    # Sample rows, selecting num_samples of each Label.\n    df = (\n        df.groupby(\"Label\")[df.columns]\n        .apply(lambda x: x.sample(num_samples))\n        .reset_index(drop=True)\n    )\n\n    df = df[df[\"Class Name\"].str.contains(classes_to_keep)]\n    df[\"Class Name\"] = df[\"Class Name\"].astype(\"category\")\n\n    return df\n\n\nTRAIN_NUM_SAMPLES = 50\nTEST_NUM_SAMPLES = 10\n# Keep rec.* and sci.*\nCLASSES_TO_KEEP = \"^rec|^sci\"\n\ndf_train = sample_data(df_train, TRAIN_NUM_SAMPLES, CLASSES_TO_KEEP)\ndf_test = sample_data(df_test, TEST_NUM_SAMPLES, CLASSES_TO_KEEP)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T12:28:22.555340Z","iopub.execute_input":"2025-04-12T12:28:22.556350Z","iopub.status.idle":"2025-04-12T12:28:22.598367Z","shell.execute_reply.started":"2025-04-12T12:28:22.556317Z","shell.execute_reply":"2025-04-12T12:28:22.597524Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Evaluate baseline performance\n\nBefore you start tuning a model, it's good practice to perform an evaluation on the available models to ensure you can measure how much the tuning helps.\n\nFirst identify a single sample row to use for visual inspection.","metadata":{}},{"cell_type":"code","source":"sample_idx = 0\nsample_row = preprocess_newsgroup_row(newsgroups_test.data[sample_idx])\nsample_label = newsgroups_test.target_names[newsgroups_test.target[sample_idx]]\n\nprint(sample_row)\nprint('---')\nprint('Label:', sample_label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T12:31:13.161326Z","iopub.execute_input":"2025-04-12T12:31:13.162396Z","iopub.status.idle":"2025-04-12T12:31:13.168231Z","shell.execute_reply.started":"2025-04-12T12:31:13.162362Z","shell.execute_reply":"2025-04-12T12:31:13.167304Z"}},"outputs":[{"name":"stdout","text":"Need info on 88-89 Bonneville\n\n\n I am a little confused on all of the models of the 88-89 bonnevilles.\nI have heard of the LE SE LSE SSE SSEI. Could someone tell me the\ndifferences are far as features or performance. I am also curious to\nknow what the book value is for prefereably the 89 model. And how much\nless than book value can you usually get them for. In other words how\nmuch are they in demand this time of year. I have heard that the mid-spring\nearly summer is the best time to buy.\n\n\t\t\tNeil Gandler\n\n---\nLabel: rec.autos\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"\n#### Fine Tune - Model\nUsing a Tuned Model for Prediction\n* The model is called with its unique ID\n* The new text is given to the model\n* The model returns the predicted label","metadata":{}},{"cell_type":"code","source":"response = client.models.generate_content(\n    model=\"gemini-1.5-flash-001\", contents=sample_row)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T12:36:33.909744Z","iopub.execute_input":"2025-04-12T12:36:33.910108Z","iopub.status.idle":"2025-04-12T12:36:37.691533Z","shell.execute_reply.started":"2025-04-12T12:36:33.910082Z","shell.execute_reply":"2025-04-12T12:36:37.690691Z"}},"outputs":[{"name":"stdout","text":"You are correct, there are a lot of Bonneville models from 1988-1989, and it can be confusing.  Here's a breakdown:\n\n**Models:**\n\n* **Bonneville:** The base model. This was the standard version with a 3.8L V6 engine and basic features. \n* **Bonneville LE (Luxury Edition):** This model included additional features like cloth upholstery, power windows, and power locks.\n* **Bonneville SE (Special Edition):** This model featured a more sporty look with unique wheels and a more powerful engine (3.8L V6 with more horsepower).\n* **Bonneville LSE (Luxury Special Edition):**  This combined features from the LE and SE, offering a more luxurious interior and a more powerful engine.\n* **Bonneville SSE (Special Performance Edition):** This model came with a 3.8L V6 engine that was tuned for more power and performance, as well as upgraded suspension and handling. \n* **Bonneville SSEi (Special Performance Edition, injection):**  This model was similar to the SSE but featured a more modern fuel-injected engine, boosting its performance. \n\n**Features and Performance:**\n\nThe main difference between the models was the level of luxury, performance, and features. The base Bonneville was the most basic, while the SSEi was the most luxurious and powerful.  \n\n**Book Value and Prices:**\n\nDetermining the book value of a 1989 Bonneville requires a reliable source.  Here are some options:\n\n* **Kelley Blue Book (KBB):**  This is a well-known resource for vehicle values. You can find a KBB estimate online or by using their app.\n* **Edmunds:** Similar to KBB, Edmunds provides vehicle valuations based on model, condition, and location.\n* **NADA Guides:**  Another popular source for vehicle pricing, NADA Guides are used by many dealers. \n\n**Finding the Right Deal:**\n\n* **Mid-Spring to Early Summer is a good time to buy.** This is generally when dealerships have more inventory and are more motivated to make deals. \n* **Negotiate:** You can typically get a good deal by negotiating with the seller. \n* **Research:** Be sure to research the model and its history before purchasing.\n* **Look for pre-owned deals:**  Consider checking online classifieds, local dealerships, and even auctions to find the best prices.\n* **Mechanical inspection:** Before you buy, have a mechanic inspect the vehicle to ensure it is in good condition.\n\n**Important Considerations:**\n\n* **Condition:** The condition of the Bonneville will significantly impact its value.  A well-maintained vehicle will command a higher price. \n* **Mileage:** Lower mileage is generally better.\n* **Availability:**  Availability can affect pricing. Some models may be more scarce than others.\n\n**A word of caution:** While the mid-spring to early summer period is a good time to buy a used car, it is not a guarantee.  Always shop around and do your research to get the best deal possible.\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"#### The Zero-Shot Learning method is used.\n* Zero-Shot Learning means that the model tries to give an appropriate answer to a new question or input without prior training on similar data.\n* Here, a prompt is sent to the model asking the model to identify which newsgroup the message belongs to.","metadata":{}},{"cell_type":"code","source":"# Ask the model directly in a zero-shot prompt.\n\nprompt = \"From what newsgroup does the following message originate?\"\nbaseline_response = client.models.generate_content(\n    model=\"gemini-1.5-flash-001\",\n    contents=[prompt, sample_row])\nprint(baseline_response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T12:43:19.255565Z","iopub.execute_input":"2025-04-12T12:43:19.255941Z","iopub.status.idle":"2025-04-12T12:43:21.096146Z","shell.execute_reply.started":"2025-04-12T12:43:19.255917Z","shell.execute_reply":"2025-04-12T12:43:21.095312Z"}},"outputs":[{"name":"stdout","text":"This message likely originates from the **rec.autos.buick** newsgroup. \n\nHere's why:\n\n* **Topic:** The message specifically discusses Buick Bonneville models from 1988-1989.\n* **Audience:**  The content and questions suggest an audience interested in Buick vehicles and their specific features. \n* **Terminology:**  The message uses terms like \"LE,\" \"SE,\" \"LSE,\" \"SSE,\" and \"SSEi,\" which are specific trim levels for Buick Bonneviles. \n* **Newsgroup Focus:** The rec.autos.buick newsgroup is a dedicated platform for discussion and information about Buick vehicles. \n\nWhile it's possible the message originated from another newsgroup related to cars in general, the specific focus on Buick Bonneville details makes rec.autos.buick the most likely source. \n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from google.api_core import retry\n\n# You can use a system instruction to do more direct prompting, and get a\n# more succinct answer.\n\nsystem_instruct = \"\"\"\nYou are a classification service. You will be passed input that represents\na newsgroup post and you must respond with the newsgroup from which the post\noriginates.\n\"\"\"\n\n# Define a helper to retry when per-minute quota is reached.\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n# If you want to evaluate your own technique, replace this body of this function\n# with your model, prompt and other code and return the predicted answer.\n@retry.Retry(predicate=is_retriable)\ndef predict_label(post: str) -> str:\n    response = client.models.generate_content(\n        model=\"gemini-1.5-flash-001\",\n        config=types.GenerateContentConfig(\n            system_instruction=system_instruct),\n        contents=post)\n\n    rc = response.candidates[0]\n\n    # Any errors, filters, recitation, etc we can mark as a general error\n    if rc.finish_reason.name != \"STOP\":\n        return \"(error)\"\n    else:\n        # Clean up the response.\n        return response.text.strip()\n\n\nprediction = predict_label(sample_row)\n\nprint(prediction)\nprint()\nprint(\"Correct!\" if prediction == sample_label else \"Incorrect.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T12:46:37.194785Z","iopub.execute_input":"2025-04-12T12:46:37.195146Z","iopub.status.idle":"2025-04-12T12:46:38.058448Z","shell.execute_reply.started":"2025-04-12T12:46:37.195121Z","shell.execute_reply":"2025-04-12T12:46:38.057548Z"}},"outputs":[{"name":"stdout","text":"rec.autos.misc\n\nIncorrect.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"#### Using tqdm and pandas, the prediction accuracy of a text classification model on sampled test data is calculated and printed.","metadata":{}},{"cell_type":"code","source":"import tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Enable tqdm features on Pandas.\ntqdmr.pandas()\n\n# But suppress the experimental warning\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n\n# Further sample the test data to be mindful of the free-tier quota.\ndf_baseline_eval = sample_data(df_test, 2, '.*')\n\n# Make predictions using the sampled data.\ndf_baseline_eval['Prediction'] = df_baseline_eval['Text'].progress_apply(predict_label)\n\n# And calculate the accuracy.\naccuracy = (df_baseline_eval[\"Class Name\"] == df_baseline_eval[\"Prediction\"]).sum() / len(df_baseline_eval)\nprint(f\"Accuracy: {accuracy:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T12:49:39.599730Z","iopub.execute_input":"2025-04-12T12:49:39.600466Z","iopub.status.idle":"2025-04-12T12:50:36.065283Z","shell.execute_reply.started":"2025-04-12T12:49:39.600438Z","shell.execute_reply":"2025-04-12T12:50:36.064399Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6a8d563a0bf4a0ea9fba3c154b20d5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"name":"stdout","text":"Accuracy: 37.50%\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"#### Now take a look at the dataframe to compare the predictions with the labels.","metadata":{}},{"cell_type":"code","source":"df_baseline_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T12:52:37.435110Z","iopub.execute_input":"2025-04-12T12:52:37.436105Z","iopub.status.idle":"2025-04-12T12:52:37.448068Z","shell.execute_reply.started":"2025-04-12T12:52:37.436069Z","shell.execute_reply":"2025-04-12T12:52:37.447128Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                                                 Text  Label  \\\n0   Re: Last of the V-8 Interceptors (Mad Max)\\n\\n...      7   \n1   Toyota Coralla 1988 FX\\n\\nFor sale, 1988 Toyot...      7   \n2   Re: Shaft-drives and Wheelies\\n\\nIn article <>...      8   \n3   Re: Countersteering_FAQ please post\\n\\nIn arti...      8   \n4   Re: Ottawa Lynx info wanted\\n\\n (CCHB) writes:...      9   \n5   RE: Game Length (was Re: Braves Update!!\\n\\n w...      9   \n6   WC, pool A\\n\\n210493\\nItaly-Sweden 2-6 (0-1,1-...     10   \n7   FAQ for this group\\n\\nCould someone please tel...     10   \n8   Re: Is it illegal to transmit encrypted data?\\...     11   \n9   Re: new encryption\\n\\n>From: \\n>              ...     11   \n10  Old schematics\\n\\nWonder if anyone would know ...     12   \n11  Re: Lead Acid batteries & Concrete?\\n\\nIn arti...     12   \n12  feverfew for migraines\\n\\n\\nI heard a short bl...     13   \n13  ROC curves software\\n\\n\\nI understand Robert C...     13   \n14  Re: Lindbergh and the moon (was:Why not give $...     14   \n15  Re: Internet resources\\n\\nIn article <>,  writ...     14   \n\n            Class Name                 Prediction  \n0            rec.autos      rec.autos.sports.cars  \n1            rec.autos                  rec.autos  \n2      rec.motorcycles            rec.motorcycles  \n3      rec.motorcycles            rec.motorcycles  \n4   rec.sport.baseball  rec.sports.baseball.minor  \n5   rec.sport.baseball        rec.sports.baseball  \n6     rec.sport.hockey           rec.sport.hockey  \n7     rec.sport.hockey           rec.sport.hockey  \n8            sci.crypt                    (error)  \n9            sci.crypt                    (error)  \n10     sci.electronics       rec.games.programmer  \n11     sci.electronics      rec.autos.maintenance  \n12             sci.med       alt.support.migraine  \n13             sci.med        comp.ai.neural-nets  \n14           sci.space           rec.autos.makers  \n15           sci.space                  sci.space  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Label</th>\n      <th>Class Name</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Re: Last of the V-8 Interceptors (Mad Max)\\n\\n...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n      <td>rec.autos.sports.cars</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Toyota Coralla 1988 FX\\n\\nFor sale, 1988 Toyot...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n      <td>rec.autos</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Re: Shaft-drives and Wheelies\\n\\nIn article &lt;&gt;...</td>\n      <td>8</td>\n      <td>rec.motorcycles</td>\n      <td>rec.motorcycles</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Re: Countersteering_FAQ please post\\n\\nIn arti...</td>\n      <td>8</td>\n      <td>rec.motorcycles</td>\n      <td>rec.motorcycles</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Re: Ottawa Lynx info wanted\\n\\n (CCHB) writes:...</td>\n      <td>9</td>\n      <td>rec.sport.baseball</td>\n      <td>rec.sports.baseball.minor</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>RE: Game Length (was Re: Braves Update!!\\n\\n w...</td>\n      <td>9</td>\n      <td>rec.sport.baseball</td>\n      <td>rec.sports.baseball</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>WC, pool A\\n\\n210493\\nItaly-Sweden 2-6 (0-1,1-...</td>\n      <td>10</td>\n      <td>rec.sport.hockey</td>\n      <td>rec.sport.hockey</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>FAQ for this group\\n\\nCould someone please tel...</td>\n      <td>10</td>\n      <td>rec.sport.hockey</td>\n      <td>rec.sport.hockey</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Re: Is it illegal to transmit encrypted data?\\...</td>\n      <td>11</td>\n      <td>sci.crypt</td>\n      <td>(error)</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Re: new encryption\\n\\n&gt;From: \\n&gt;              ...</td>\n      <td>11</td>\n      <td>sci.crypt</td>\n      <td>(error)</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Old schematics\\n\\nWonder if anyone would know ...</td>\n      <td>12</td>\n      <td>sci.electronics</td>\n      <td>rec.games.programmer</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Re: Lead Acid batteries &amp; Concrete?\\n\\nIn arti...</td>\n      <td>12</td>\n      <td>sci.electronics</td>\n      <td>rec.autos.maintenance</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>feverfew for migraines\\n\\n\\nI heard a short bl...</td>\n      <td>13</td>\n      <td>sci.med</td>\n      <td>alt.support.migraine</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>ROC curves software\\n\\n\\nI understand Robert C...</td>\n      <td>13</td>\n      <td>sci.med</td>\n      <td>comp.ai.neural-nets</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Re: Lindbergh and the moon (was:Why not give $...</td>\n      <td>14</td>\n      <td>sci.space</td>\n      <td>rec.autos.makers</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Re: Internet resources\\n\\nIn article &lt;&gt;,  writ...</td>\n      <td>14</td>\n      <td>sci.space</td>\n      <td>sci.space</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## Tune a custom model\n\nIn this example you'll use tuning to create a model that requires no prompting or system instructions and outputs succinct text from the classes you provide in the training data.\n\nThe data contains both input text (the processed posts) and output text (the category, or newsgroup), that you can use to start tuning a model.\n\nWhen calling `tune()`, you can specify model tuning hyperparameters too:\n - `epoch_count`: defines how many times to loop through the data,\n - `batch_size`: defines how many rows to process in a single step, and\n - `learning_rate`: defines the scaling factor for updating model weights at each step.\n\nYou can also choose to omit them and use the defaults. [Learn more](https://developers.google.com/machine-learning/crash-course/linear-regression/hyperparameters) about these parameters and how they work. For this example these parameters were selected by running some tuning jobs and selecting parameters that converged efficiently.\n\nThis example will start a new tuning job, but only if one does not already exist. This allows you to leave this codelab and come back later - re-running this step will find your last model.","metadata":{}},{"cell_type":"code","source":"from collections.abc import Iterable\nimport random\n\n\n# Convert the data frame into a dataset suitable for tuning.\ninput_data = {'examples': \n    df_train[['Text', 'Class Name']]\n      .rename(columns={'Text': 'textInput', 'Class Name': 'output'})\n      .to_dict(orient='records')\n }\n\n# If you are re-running this lab, add your model_id here.\nmodel_id = None\n\n# Or try and find a recent tuning job.\nif not model_id:\n  queued_model = None\n  # Newest models first.\n  for m in reversed(client.tunings.list()):\n    # Only look at newsgroup classification models.\n    if m.name.startswith('tunedModels/newsgroup-classification-model'):\n      # If there is a completed model, use the first (newest) one.\n      if m.state.name == 'JOB_STATE_SUCCEEDED':\n        model_id = m.name\n        print('Found existing tuned model to reuse.')\n        break\n\n      elif m.state.name == 'JOB_STATE_RUNNING' and not queued_model:\n        # If there's a model still queued, remember the most recent one.\n        queued_model = m.name\n  else:\n    if queued_model:\n      model_id = queued_model\n      print('Found queued model, still waiting.')\n\n\n# Upload the training data and queue the tuning job.\nif not model_id:\n    tuning_op = client.tunings.tune(\n        base_model=\"models/gemini-1.5-flash-001-tuning\",\n        training_dataset=input_data,\n        config=types.CreateTuningJobConfig(\n            tuned_model_display_name=\"Newsgroup classification model\",\n            batch_size=16,\n            epoch_count=2,\n        ),\n    )\n\n    print(tuning_op.state)\n    model_id = tuning_op.name\n\nprint(model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T12:57:36.710301Z","iopub.execute_input":"2025-04-12T12:57:36.710689Z","iopub.status.idle":"2025-04-12T12:57:37.515123Z","shell.execute_reply.started":"2025-04-12T12:57:36.710661Z","shell.execute_reply":"2025-04-12T12:57:37.514294Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2550230802.py:39: ExperimentalWarning: The SDK's tuning implementation is experimental, and may change in future versions.\n  tuning_op = client.tunings.tune(\n","output_type":"stream"},{"name":"stdout","text":"JobState.JOB_STATE_QUEUED\ntunedModels/newsgroup-classification-model-dbf0p8g7g\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"The model is training in the background, which may take a while, but you don't have to wait or keep the app open—just save the model ID to come back and use it later.","metadata":{}},{"cell_type":"code","source":"import datetime\nimport time\n\n\nMAX_WAIT = datetime.timedelta(minutes=10)\n\nwhile not (tuned_model := client.tunings.get(name=model_id)).has_ended:\n\n    print(tuned_model.state)\n    time.sleep(60)\n\n    # Don't wait too long. Use a public model if this is going to take a while.\n    if datetime.datetime.now(datetime.timezone.utc) - tuned_model.create_time > MAX_WAIT:\n        print(\"Taking a shortcut, using a previously prepared model.\")\n        model_id = \"tunedModels/newsgroup-classification-model-ltenbi1b\"\n        tuned_model = client.tunings.get(name=model_id)\n        break\n\n\nprint(f\"Done! The model state is: {tuned_model.state.name}\")\n\nif not tuned_model.has_succeeded and tuned_model.error:\n    print(\"Error:\", tuned_model.error)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:00:01.930133Z","iopub.execute_input":"2025-04-12T13:00:01.930538Z","iopub.status.idle":"2025-04-12T13:02:02.998136Z","shell.execute_reply.started":"2025-04-12T13:00:01.930512Z","shell.execute_reply":"2025-04-12T13:02:02.997317Z"}},"outputs":[{"name":"stdout","text":"JobState.JOB_STATE_RUNNING\nJobState.JOB_STATE_RUNNING\nDone! The model state is: JOB_STATE_SUCCEEDED\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Use the new model\n\nNow that you have a tuned model, try it out with custom data. You use the same API as a normal Gemini API interaction, but you specify your new model as the model name, which will start with `tunedModels/`.","metadata":{}},{"cell_type":"code","source":"new_text = \"\"\"\nFirst-timer looking to get out of here.\n\nHi, I'm writing about my interest in travelling to the outer limits!\n\nWhat kind of craft can I buy? What is easiest to access from this 3rd rock?\n\nLet me know how to do that please.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model=model_id, contents=new_text)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:02:02.999850Z","iopub.execute_input":"2025-04-12T13:02:03.000606Z","iopub.status.idle":"2025-04-12T13:02:05.393838Z","shell.execute_reply.started":"2025-04-12T13:02:03.000578Z","shell.execute_reply":"2025-04-12T13:02:05.392864Z"}},"outputs":[{"name":"stdout","text":"sci.space\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"### Evaluation¶\nwe can see that the model outputs labels that correspond to those in the training data, and without any system instructions or prompting, which is already a great improvement. Now see how well it performs on the test set.","metadata":{}},{"cell_type":"code","source":"@retry.Retry(predicate=is_retriable)\ndef classify_text(text: str) -> str:\n    \"\"\"Classify the provided text into a known newsgroup.\"\"\"\n    response = client.models.generate_content(\n        model=model_id, contents=text)\n    rc = response.candidates[0]\n\n    # Any errors, filters, recitation, etc we can mark as a general error\n    if rc.finish_reason.name != \"STOP\":\n        return \"(error)\"\n    else:\n        return rc.content.parts[0].text\n\n\n# The sampling here is just to minimise your quota usage. If you can, you should\n# evaluate the whole test set with `df_model_eval = df_test.copy()`.\ndf_model_eval = sample_data(df_test, 4, '.*')\n\ndf_model_eval[\"Prediction\"] = df_model_eval[\"Text\"].progress_apply(classify_text)\n\naccuracy = (df_model_eval[\"Class Name\"] == df_model_eval[\"Prediction\"]).sum() / len(df_model_eval)\nprint(f\"Accuracy: {accuracy:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:03:55.836045Z","iopub.execute_input":"2025-04-12T13:03:55.836417Z","iopub.status.idle":"2025-04-12T13:05:01.369902Z","shell.execute_reply.started":"2025-04-12T13:03:55.836393Z","shell.execute_reply":"2025-04-12T13:05:01.369003Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74cb3f0a10c44332be1a60fedb53eafe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"name":"stdout","text":"Accuracy: 93.75%\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## Compare token usage\n\nAI Studio and the Gemini API provide model tuning at no cost, however normal limits and charges apply for *use* of a tuned model.\n\nThe size of the input prompt and other generation config like system instructions, as well as the number of generated output tokens, all contribute to the overall cost of a request.","metadata":{}},{"cell_type":"code","source":"# Calculate the *input* cost of the baseline model with system instructions.\nsysint_tokens = client.models.count_tokens(\n    model='gemini-1.5-flash-001', contents=[system_instruct, sample_row]\n).total_tokens\nprint(f'System instructed baseline model: {sysint_tokens} (input)')\n\n# Calculate the input cost of the tuned model.\ntuned_tokens = client.models.count_tokens(model=tuned_model.base_model, contents=sample_row).total_tokens\nprint(f'Tuned model: {tuned_tokens} (input)')\n\nsavings = (sysint_tokens - tuned_tokens) / tuned_tokens\nprint(f'Token savings: {savings:.2%}')  # Note that this is only n=1.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:06:02.940351Z","iopub.execute_input":"2025-04-12T13:06:02.940864Z","iopub.status.idle":"2025-04-12T13:06:03.255025Z","shell.execute_reply.started":"2025-04-12T13:06:02.940816Z","shell.execute_reply":"2025-04-12T13:06:03.253392Z"}},"outputs":[{"name":"stdout","text":"System instructed baseline model: 172 (input)\nTuned model: 136 (input)\nToken savings: 26.47%\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"The earlier verbose model also produced more output tokens than needed for this task.","metadata":{}},{"cell_type":"code","source":"baseline_token_output = baseline_response.usage_metadata.candidates_token_count\nprint('Baseline (verbose) output tokens:', baseline_token_output)\n\ntuned_model_output = client.models.generate_content(\n    model=model_id, contents=sample_row)\ntuned_tokens_output = tuned_model_output.usage_metadata.candidates_token_count\nprint('Tuned output tokens:', tuned_tokens_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:07:16.204815Z","iopub.execute_input":"2025-04-12T13:07:16.205608Z","iopub.status.idle":"2025-04-12T13:07:17.076569Z","shell.execute_reply.started":"2025-04-12T13:07:16.205549Z","shell.execute_reply":"2025-04-12T13:07:17.075751Z"}},"outputs":[{"name":"stdout","text":"Baseline (verbose) output tokens: 179\nTuned output tokens: 3\n","output_type":"stream"}],"execution_count":24}]}